{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Reference : https://www.kaggle.com/jannesklaas/a-simple-seq2seq-translator/notebook\n",
    "-             https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "- 그림출처 : https://towardsdatascience.com/neural-machine-translation-using-seq2seq-with-keras-c23540453c74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras English to French translator 만들기\n",
    "- 우리가 구성할 모델은 seqeunce-2-sequence 모듈을 이용하여 영어 문장을 프랑스어 문장으로 바꿔주는 기계 번역기를 만드는 것입니다.\n",
    "- 본 실습에서는 기본적인 seq2seq 모듈과 attention 방식을 적용한 2가지 발전된 seq2seq모듈을 구현해보고자 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# keras를 비롯하여 필요한 모듈들을 불러옵니다.\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64   # Batch size for training.\n",
    "epochs = 25       # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = './fra-eng/fra.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 예시\n",
    "- 아래 표는 model training에 사용될 문구들의 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Go.</th>\n",
       "      <th>Va !</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fire!</td>\n",
       "      <td>Au feu !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Help!</td>\n",
       "      <td>À l'aide !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jump.</td>\n",
       "      <td>Saute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>Ça suffit !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>Stop !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>Arrête-toi !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>Attends !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>Attendez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I see.</td>\n",
       "      <td>Je comprends.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I try.</td>\n",
       "      <td>J'essaye.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I won!</td>\n",
       "      <td>J'ai gagné !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I won!</td>\n",
       "      <td>Je l'ai emporté !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Oh no!</td>\n",
       "      <td>Oh non !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Attack!</td>\n",
       "      <td>Attaque !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Attack!</td>\n",
       "      <td>Attaquez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cheers!</td>\n",
       "      <td>Santé !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Cheers!</td>\n",
       "      <td>À votre santé !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Go.               Va !\n",
       "0      Run!            Cours !\n",
       "1      Run!           Courez !\n",
       "2      Wow!         Ça alors !\n",
       "3     Fire!           Au feu !\n",
       "4     Help!         À l'aide !\n",
       "5     Jump.             Saute.\n",
       "6     Stop!        Ça suffit !\n",
       "7     Stop!             Stop !\n",
       "8     Stop!       Arrête-toi !\n",
       "9     Wait!          Attends !\n",
       "10    Wait!         Attendez !\n",
       "11   I see.      Je comprends.\n",
       "12   I try.          J'essaye.\n",
       "13   I won!       J'ai gagné !\n",
       "14   I won!  Je l'ai emporté !\n",
       "15   Oh no!           Oh non !\n",
       "16  Attack!          Attaque !\n",
       "17  Attack!         Attaquez !\n",
       "18  Cheers!            Santé !\n",
       "19  Cheers!    À votre santé !"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_path,delimiter='\\t')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df # Save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n"
     ]
    }
   ],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "lines = open(data_path, 'rt', encoding='UTF8').read().split('\\n')\n",
    "# 모든 pair 중 앞의 10000쌍 만 골라서 학습에 사용하도록 하겠습니다.\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    # Input (English)과 target (French)을 분리합니다.\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    \n",
    "    # Target text에서 \"tab\"을 \"start sequence\" character로, \"\\n\" as \"end sequence\" character로 둡니다.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    # input, output 문장들을 구성하는 token들의 집합\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "            \n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "print('Number of samples:', len(input_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문자의 벡터화\n",
    "- 모델을 트레이닝 하기 위해서 입력된 문자들을 벡터 형태로 바꿔주는 것이 필요합니다.\n",
    "- 문자를 해당하는 벡터로 바꾸는 방식은 여러 가지 방법이 있지만, 여기서는 하나의 위치만 1을 나타내고 나머지는 0인 one-hot vector로 encoding하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 93\n"
     ]
    }
   ],
   "source": [
    "# Input과 target 문자들을 순서대로 정렬해줍니다.\n",
    "input_characters = sorted(list(input_characters)) \n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)  # aka size of the english alphabet + numbers, signs, etc.\n",
    "num_decoder_tokens = len(target_characters) # aka size of the french alphabet + numbers, signs, etc.\n",
    "\n",
    "# one-hot encoding으로 생성한 vector의 dimension이 됩니다.\n",
    "# 즉 여기서는 input (알파벳 및 문장부호)의 경우 71 dimension의 vector, output (프랑스어 알파벳 및 문장부호)은 93 vector dimension의 vector로 바꿀 것입니다.\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 51 48 0 46 44 63 0 62 52 63 62 0 58 57 0 63 51 48 0 56 44 63 "
     ]
    }
   ],
   "source": [
    "# 각 token에 대응하는 숫자를 생성합니다.\n",
    "input_token_index = {char: i for i, char in enumerate(input_characters)}\n",
    "target_token_index = {char: i for i, char in enumerate(target_characters)}\n",
    "for c in 'the cat sits on the mat':\n",
    "    print(input_token_index[c], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "# 사용할 데이터 중 가장 긴 문구의 길이(글자수)를 가져옵니다.\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts]) \n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input_data는 (num_pairs, max_english_sentence_length, num_english_characters) 형태의 3D array\n",
    "# 즉 영어문장의 one-hot vector들로 구성됩니다.\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# decoder_input_data 역시 (num_pairs, max_french_sentence_length, num_french_characters) 형태의 3D array\n",
    "# 프랑스어 문장의 one-hot vector들로 구성됩니다.\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# decoder_target_data는 input과 형태는 같지만 input에서 한 time step 뒤의 값을 가집니다.\n",
    "# 예를 들어 'abc'라는 decoder input sequence가 있다면, target data는 기존 값에서 모델을 통해 예측된 문자 'd'를 덧붙인 'abcd'가 될 것입니다.\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 내 각 token에 대응하는 One-hot 벡터를 만드는 과정입니다.\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data는 \n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구성하기\n",
    "\n",
    "- 아래 코드에 training을 위한 모델을 소개하였습니다.\n",
    "- 성능을 높이기 위해 decoder 단에서 입력된 target 값에서 한 time step 씩 뒤의 값들을 decoder target 값으로 사용합니다. 즉 decoder input/output에서 모두 labeled 된 값을 사용합니다.\n",
    "![seq2seq_training](./seq2seq_training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_train(num_encoder_tokens, num_decoder_tokens, latent_dim):\n",
    "    # Encoder\n",
    "    # return_state argument를 True로 설정하여 LSTN layer의 마지막 time step의 내부 state들을 얻습니다.\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens), \n",
    "                           name = 'encoder_inputs')\n",
    "    encoder = LSTM(latent_dim, \n",
    "                        return_state=True, \n",
    "                        name = 'encoder')\n",
    "\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Decoder \n",
    "    # encoder_states를 decoder 단의 lstm에 초기 state로 이용합니다.\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens), \n",
    "                           name = 'decoder_inputs')\n",
    "\n",
    "    decoder_lstm = LSTM(latent_dim, \n",
    "                             return_sequences=True, \n",
    "                             return_state=True, \n",
    "                             name = 'decoder_lstm')\n",
    "\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "\n",
    "    decoder_dense = Dense(num_decoder_tokens, \n",
    "                          activation='softmax', \n",
    "                          name = 'decoder_dense')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    return encoder_inputs, encoder_states, decoder_inputs, decoder_lstm, decoder_dense, model\n",
    "[encoder_inputs, encoder_states, decoder_inputs, decoder_lstm, decoder_dense, model] = seq2seq_train(num_encoder_tokens, num_decoder_tokens, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델을 불러오는 부분입니다.\n",
    "project_path = './model/'\n",
    "\n",
    "if not os.path.exists(project_path):\n",
    "    os.mkdir(project_path)\n",
    "    \n",
    "model_path = project_path + 's2s.hdf5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/25\n",
      "8000/8000 [==============================] - 70s 9ms/step - loss: 0.7712 - val_loss: 0.8086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/network.py:872: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'encoder/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "8000/8000 [==============================] - 68s 9ms/step - loss: 0.6406 - val_loss: 0.7304\n",
      "Epoch 3/25\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 0.5777 - val_loss: 0.6821\n",
      "Epoch 4/25\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 0.5337 - val_loss: 0.6371\n",
      "Epoch 5/25\n",
      "8000/8000 [==============================] - 67s 8ms/step - loss: 0.4995 - val_loss: 0.6133\n",
      "Epoch 6/25\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 0.4716 - val_loss: 0.5871\n",
      "Epoch 7/25\n",
      "8000/8000 [==============================] - 70s 9ms/step - loss: 0.4494 - val_loss: 0.5688\n",
      "Epoch 8/25\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 0.4302 - val_loss: 0.5500\n",
      "Epoch 9/25\n",
      "8000/8000 [==============================] - 72s 9ms/step - loss: 0.4125 - val_loss: 0.5425\n",
      "Epoch 10/25\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 0.3969 - val_loss: 0.5347\n",
      "Epoch 11/25\n",
      "8000/8000 [==============================] - 70s 9ms/step - loss: 0.3827 - val_loss: 0.5221\n",
      "Epoch 12/25\n",
      "8000/8000 [==============================] - 70s 9ms/step - loss: 0.3693 - val_loss: 0.5129\n",
      "Epoch 13/25\n",
      "8000/8000 [==============================] - 70s 9ms/step - loss: 0.3569 - val_loss: 0.5080\n",
      "Epoch 14/25\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 0.3450 - val_loss: 0.5008\n",
      "Epoch 15/25\n",
      "8000/8000 [==============================] - 70s 9ms/step - loss: 0.3338 - val_loss: 0.4927\n",
      "Epoch 16/25\n",
      "8000/8000 [==============================] - 71s 9ms/step - loss: 0.3241 - val_loss: 0.4923\n",
      "Epoch 17/25\n",
      "8000/8000 [==============================] - 70s 9ms/step - loss: 0.3139 - val_loss: 0.4905\n",
      "Epoch 18/25\n",
      "8000/8000 [==============================] - 70s 9ms/step - loss: 0.3041 - val_loss: 0.4905\n",
      "Epoch 19/25\n",
      "8000/8000 [==============================] - 70s 9ms/step - loss: 0.2954 - val_loss: 0.4824\n",
      "Epoch 20/25\n",
      "8000/8000 [==============================] - 70s 9ms/step - loss: 0.2865 - val_loss: 0.4844\n",
      "Epoch 21/25\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 0.2778 - val_loss: 0.4859\n",
      "Epoch 22/25\n",
      "8000/8000 [==============================] - 68s 9ms/step - loss: 0.2700 - val_loss: 0.4845\n",
      "Epoch 23/25\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 0.2618 - val_loss: 0.4889\n",
      "Epoch 24/25\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 0.2547 - val_loss: 0.4849\n",
      "Epoch 25/25\n",
      "8000/8000 [==============================] - 70s 9ms/step - loss: 0.2477 - val_loss: 0.4852\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습하기 (시간상 넘어가도록 하겠습니다.)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "history = model.fit([encoder_input_data, decoder_input_data], \n",
    "                    decoder_target_data,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[checkpoint])\n",
    "# 학습한 모델 저장하기\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-fb56e49dc6c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Training curve를 그리는 코드입니다. 모델이 트레이닝 될 경우에만 그려주므로 넘어가도록 합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Validation Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training curve를 그리는 코드입니다. 모델이 트레이닝 될 경우에만 그려주므로 넘어가도록 합니다.\n",
    "plt.figure(figsize=(10,7))\n",
    "a, = plt.plot(history.history['loss'],label='Training Loss')\n",
    "b, = plt.plot(history.history['val_loss'],label='Validation Loss')\n",
    "plt.legend(handles=[a,b])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Curve](./seq2seq_training_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference (sampling) model 만들기\n",
    "- 실제 우리가 사용할 모델은 decoder단에서 예측한 문자가 다시 lstm의 input으로 들어가 다음 글자를 예측하는 과정이 반복되는 방식을 사용합니다. \n",
    "![seq2seq_inference](./seq2seq_inference.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_path)\n",
    "def seq2seq_inference(latent_dim, encoder_inputs, encoder_states, decoder_inputs, decoder_lstm, decoder_dense):\n",
    "    '''Encoder model'''\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    '''Decoder model'''\n",
    "    # Decoder의 input\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    # Decoder output 및 state\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    # 다음 문자 예측\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Decoder 모델\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    return [encoder_model, decoder_model]\n",
    "[encoder_model, decoder_model] = seq2seq_inference(latent_dim, encoder_inputs, encoder_states, decoder_inputs, decoder_lstm, decoder_dense)\n",
    "# [encoder_model, decoder_model] = seq2seq(num_encoder_tokens, num_decoder_tokens, latent_dim).model_inference(encoder_inputs, encoder_states, decoder_inputs, decoder_lstm, decoder_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' ', 1: '!', 2: '$', 3: '&', 4: \"'\", 5: ',', 6: '-', 7: '.', 8: '0', 9: '1', 10: '2', 11: '3', 12: '4', 13: '5', 14: '6', 15: '7', 16: '9', 17: ':', 18: '?', 19: 'A', 20: 'B', 21: 'C', 22: 'D', 23: 'E', 24: 'F', 25: 'G', 26: 'H', 27: 'I', 28: 'J', 29: 'K', 30: 'L', 31: 'M', 32: 'N', 33: 'O', 34: 'P', 35: 'Q', 36: 'R', 37: 'S', 38: 'T', 39: 'U', 40: 'V', 41: 'W', 42: 'Y', 43: 'Z', 44: 'a', 45: 'b', 46: 'c', 47: 'd', 48: 'e', 49: 'f', 50: 'g', 51: 'h', 52: 'i', 53: 'j', 54: 'k', 55: 'l', 56: 'm', 57: 'n', 58: 'o', 59: 'p', 60: 'q', 61: 'r', 62: 's', 63: 't', 64: 'u', 65: 'v', 66: 'w', 67: 'x', 68: 'y', 69: 'z', 70: '’'}\n",
      "{0: '\\t', 1: '\\n', 2: ' ', 3: '!', 4: '$', 5: '&', 6: \"'\", 7: '(', 8: ')', 9: ',', 10: '-', 11: '.', 12: '0', 13: '1', 14: '5', 15: '6', 16: '9', 17: ':', 18: '?', 19: 'A', 20: 'B', 21: 'C', 22: 'D', 23: 'E', 24: 'F', 25: 'G', 26: 'H', 27: 'I', 28: 'J', 29: 'K', 30: 'L', 31: 'M', 32: 'N', 33: 'O', 34: 'P', 35: 'Q', 36: 'R', 37: 'S', 38: 'T', 39: 'U', 40: 'V', 41: 'Y', 42: 'Z', 43: 'a', 44: 'b', 45: 'c', 46: 'd', 47: 'e', 48: 'f', 49: 'g', 50: 'h', 51: 'i', 52: 'j', 53: 'k', 54: 'l', 55: 'm', 56: 'n', 57: 'o', 58: 'p', 59: 'q', 60: 'r', 61: 's', 62: 't', 63: 'u', 64: 'v', 65: 'w', 66: 'x', 67: 'y', 68: 'z', 69: '\\xa0', 70: '«', 71: '»', 72: 'À', 73: 'Ç', 74: 'É', 75: 'Ê', 76: 'à', 77: 'â', 78: 'ç', 79: 'è', 80: 'é', 81: 'ê', 82: 'ë', 83: 'î', 84: 'ï', 85: 'ô', 86: 'ù', 87: 'û', 88: 'œ', 89: '\\u2009', 90: '‘', 91: '’', 92: '\\u202f'}\n"
     ]
    }
   ],
   "source": [
    "# decode된 각 숫자에 대응하는 token입니다.\n",
    "reverse_input_char_index = {i: char \n",
    "                            for char, i in input_token_index.items()}\n",
    "reverse_target_char_index = {i: char \n",
    "                             for char, i in target_token_index.items()}\n",
    "print(reverse_input_char_index)\n",
    "print(reverse_target_char_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문자 decoding\n",
    "이제 학습한 모델을 이용해 영어문장을 프랑스어 문장으로 바꿔봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Input을 state vector들로 encode 합니다.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Decoder input sequence의 첫 부분을 start character로 초기화합니다.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    # stop sign이 나오기 전까지 문장을 예측합니다.\n",
    "    while not stop_condition:\n",
    "        # Decoder의 이전 output, internal states를 이용해 다음 값을 예측합니다.\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # 가장 높은 확률로 예측되는 token을 sampling합니다.\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # 예측한 token에 해당하는 문자를 찾습니다.\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        # decoded된 문장에 예측 문자를 덧붙여 문장을 예측해갑니다.\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # decoded된 문장이 최대 길이를 초과하거나 'stop' character를 만나게 될 때 예측을 멈춥니다.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Target sequence를 update합니다.\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 다음 문자를 예측하기 위해 state 값을 update합니다.\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = 'Run!'\n",
    "placeholder = np.zeros((1,len(my_text)+10,num_encoder_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R 36\n",
      "1 u 64\n",
      "2 n 57\n",
      "3 ! 1\n"
     ]
    }
   ],
   "source": [
    "for i, char in enumerate(my_text):\n",
    "    print(i,char, input_token_index[char])\n",
    "    placeholder[0,i,input_token_index[char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'àzjô’’ûôf???TZTZTêê\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence(placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Va chercher se te prier.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Dépêchez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Dépêchez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Comme c'est cela !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Sais-tu la détente !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Ve vous aider vous les foits !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Détends-toi un vous !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Prends un biscuit !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Prends un biscuit !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Prends un biscuit !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
